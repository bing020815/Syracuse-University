{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping\n",
    "* Outline:\n",
    "    1. Web scraping using Python and BeautifulSoup - block 1\n",
    "        - US presidents in history from wikipedia\n",
    "            + requests\n",
    "            + BeautifulSoup\n",
    "            + or urllib.request\n",
    "    2. Web Scraping Wikipedia Tables using BeautifulSoup and Python - block 12\n",
    "        - Countries listed in the table from wikipedia\n",
    "            + requests\n",
    "            + BeautifulSoup\n",
    "            + Pandas\n",
    "    3. Web Scraping HTML Tables with Python - block 21\n",
    "        - Table from the Pokemon Database\n",
    "            + requests\n",
    "            + lxml.html\n",
    "            + Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping using Python and BeautifulSoup\n",
    "https://www.codementor.io/dankhan/web-scrapping-using-python-and-beautifulsoup-o3hxadit4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requests and beautifulsoup4\n",
    "# $ pip install requests\n",
    "# $ pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between requests.get() and urrlib.request.urlopen() python:  \n",
    "https://stackoverflow.com/questions/38114499/difference-between-requests-get-and-urrlib-request-urlopen-python  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### urllib and urllib2 are both Python modules that do URL request related stuff but offer different functionalities.  \n",
    "\n",
    "1) urllib2 can accept a Request object to set the headers for a URL request, urllib accepts only a URL.  \n",
    "\n",
    "2) urllib provides the urlencode method which is used for the generation of GET query strings, urllib2 doesn't have such a function. This is one of the reasons why urllib is often used along with urllib2.  \n",
    "\n",
    "##### Requests - Requests’ is a simple, easy-to-use HTTP library written in Python.  \n",
    "\n",
    "1) Python Requests encodes the parameters automatically so you just pass them as simple arguments, unlike in the case of urllib, where you need to use the method urllib.encode() to encode the parameters before passing them.  \n",
    "\n",
    "2) It automatically decoded the response into Unicode.  \n",
    "\n",
    "3) Requests also has far more convenient error handling.If your authentication failed, urllib2 would raise a urllib2.URLError, while Requests would return a normal response object, as expected. All you have to see if the request was successful by boolean response.ok  \n",
    "\n",
    "Reference - https://stackoverflow.com/questions/2018026/what-are-the-differences-between-the-urllib-urllib2-urllib3-and-requests-modul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting web page data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to this link, https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States#Presidents, and right click on the table containing all the information about the United States presidents and then click on the inspect to inspect the page\n",
    "\n",
    "      # The goal for this exercise is to find all the United State Presidents in the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the installed modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the data from the web page we will use requests API's get() method\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States\"\n",
    "page = requests.get(url)\n",
    "#page = request.urlopen(url).read().decode('utf8')  # use urllib.request module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# page is a requests.models.Response object if we use Requests module\n",
    "# page is a string object if we use urllib.Request module\n",
    "type(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of the response status code:\n",
    "https://developer.mozilla.org/en-US/docs/Web/HTTP/Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# It is always good to check the http response status code from Requests \n",
    "print(page.status_code)   # This should print 200\n",
    "# print(page[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>List of presidents of the United States - Wikipedia</title>\\n<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRequestId\":\"XciHFgpAIDEAABQJKH4AAACM\",\"wgCSPNonce\":!1,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"List_of_presidents_of_the_United_States\",\"wgTitle\":\"List of presidents of the Unite'\n"
     ]
    }
   ],
   "source": [
    "# Now we have collected the data from the web page by using content() method from Requests module\n",
    "print(page.content[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   List of presidents of the United States - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRequestId\":\"XciHFgpAIDEAABQJKH4AAACM\",\"wgCSPNonce\":!1,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"List_of_presidents_of_the_United_States\",\"wgTitle\":\"List of presidents of the United States\",\"wgCurRevisionId\":925557458,\"wgRevisionId\":925557458,\"wgArticleId\":19908980,\"wgIsArticle\":!0,\"wgIsRedirect\":!1,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgC\n"
     ]
    }
   ],
   "source": [
    "# create a bs4 object and use the prettify method from bs4\n",
    "# This will print data in format like inspecting the web page.\n",
    "soup = BeautifulSoup(page.content, 'html.parser') # the input of the BeautifulSoup should be string object or bytes\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now we know that our table is in tag \"table\" and class \"wikitable\".   \n",
    "So, first we will extract the data in table tag using find method of bs4 object.   \n",
    "This method returns a bs4 object  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = soup.find('table', {'class':'wikitable'}) # find tag, table, and class (using dictionary), wikitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tb should be a bs4.element.Tag object\n",
    "type(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tag has many nested tags but we only need text under title element of the tag a of parent tag b (which is the child tag of table).   \n",
    "For that we need to find all b tags under the table tag and then find all the a tags under the b tags.  \n",
    "For this we will use find_all method and iterate over each of the b tag to get the a tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML < b > tag is used to create a 'b' element, which represents bold text in an HTML document.  \n",
    "   \n",
    "The HTML < a > tag is used for creating an a element (also known as an \"anchor\" element). The a element represents a hyperlink. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/wiki/George_Washington\" title=\"George Washington\">George Washington</a>\n",
      "<a href=\"/wiki/John_Adams\" title=\"John Adams\">John Adams</a>\n",
      "<a href=\"/wiki/Thomas_Jefferson\" title=\"Thomas Jefferson\">Thomas Jefferson</a>\n",
      "<a href=\"/wiki/James_Madison\" title=\"James Madison\">James Madison</a>\n",
      "<a href=\"/wiki/James_Monroe\" title=\"James Monroe\">James Monroe</a>\n",
      "<a href=\"/wiki/John_Quincy_Adams\" title=\"John Quincy Adams\">John Quincy Adams</a>\n",
      "<a href=\"/wiki/Andrew_Jackson\" title=\"Andrew Jackson\">Andrew Jackson</a>\n",
      "<a href=\"/wiki/Martin_Van_Buren\" title=\"Martin Van Buren\">Martin Van Buren</a>\n",
      "<a href=\"/wiki/William_Henry_Harrison\" title=\"William Henry Harrison\">William Henry Harrison</a>\n",
      "<a href=\"/wiki/John_Tyler\" title=\"John Tyler\">John Tyler</a>\n",
      "<a href=\"/wiki/James_K._Polk\" title=\"James K. Polk\">James K. Polk</a>\n",
      "<a href=\"/wiki/Zachary_Taylor\" title=\"Zachary Taylor\">Zachary Taylor</a>\n",
      "<a href=\"/wiki/Millard_Fillmore\" title=\"Millard Fillmore\">Millard Fillmore</a>\n",
      "<a href=\"/wiki/Franklin_Pierce\" title=\"Franklin Pierce\">Franklin Pierce</a>\n",
      "<a href=\"/wiki/James_Buchanan\" title=\"James Buchanan\">James Buchanan</a>\n",
      "<a href=\"/wiki/Abraham_Lincoln\" title=\"Abraham Lincoln\">Abraham Lincoln</a>\n",
      "<a href=\"/wiki/Andrew_Johnson\" title=\"Andrew Johnson\">Andrew Johnson</a>\n",
      "<a href=\"/wiki/Ulysses_S._Grant\" title=\"Ulysses S. Grant\">Ulysses S. Grant</a>\n",
      "<a href=\"/wiki/Rutherford_B._Hayes\" title=\"Rutherford B. Hayes\">Rutherford B. Hayes</a>\n",
      "<a href=\"/wiki/James_A._Garfield\" title=\"James A. Garfield\">James A. Garfield</a>\n",
      "<a href=\"/wiki/Chester_A._Arthur\" title=\"Chester A. Arthur\">Chester A. Arthur</a>\n",
      "<a href=\"/wiki/Grover_Cleveland\" title=\"Grover Cleveland\">Grover Cleveland</a>\n",
      "<a href=\"/wiki/Benjamin_Harrison\" title=\"Benjamin Harrison\">Benjamin Harrison</a>\n",
      "<a href=\"/wiki/Grover_Cleveland\" title=\"Grover Cleveland\">Grover Cleveland</a>\n",
      "<a href=\"/wiki/William_McKinley\" title=\"William McKinley\">William McKinley</a>\n",
      "<a href=\"/wiki/Theodore_Roosevelt\" title=\"Theodore Roosevelt\">Theodore Roosevelt</a>\n",
      "<a href=\"/wiki/William_Howard_Taft\" title=\"William Howard Taft\">William H. Taft</a>\n",
      "<a href=\"/wiki/Woodrow_Wilson\" title=\"Woodrow Wilson\">Woodrow Wilson</a>\n",
      "<a href=\"/wiki/Warren_G._Harding\" title=\"Warren G. Harding\">Warren Harding</a>\n",
      "<a href=\"/wiki/Calvin_Coolidge\" title=\"Calvin Coolidge\">Calvin Coolidge</a>\n",
      "<a href=\"/wiki/Herbert_Hoover\" title=\"Herbert Hoover\">Herbert Hoover</a>\n",
      "<a href=\"/wiki/Franklin_D._Roosevelt\" title=\"Franklin D. Roosevelt\">Franklin Delano Roosevelt</a>\n",
      "<a href=\"/wiki/Harry_S._Truman\" title=\"Harry S. Truman\">Harry S. Truman</a>\n",
      "<a href=\"/wiki/Dwight_D._Eisenhower\" title=\"Dwight D. Eisenhower\">Dwight D. Eisenhower</a>\n",
      "<a href=\"/wiki/John_F._Kennedy\" title=\"John F. Kennedy\">John F. Kennedy</a>\n",
      "<a href=\"/wiki/Lyndon_B._Johnson\" title=\"Lyndon B. Johnson\">Lyndon B. Johnson</a>\n",
      "<a href=\"/wiki/Richard_Nixon\" title=\"Richard Nixon\">Richard Nixon</a>\n",
      "<a href=\"/wiki/Gerald_Ford\" title=\"Gerald Ford\">Gerald Ford</a>\n",
      "<a href=\"/wiki/Jimmy_Carter\" title=\"Jimmy Carter\">Jimmy Carter</a>\n",
      "<a href=\"/wiki/Ronald_Reagan\" title=\"Ronald Reagan\">Ronald Reagan</a>\n",
      "<a href=\"/wiki/George_H._W._Bush\" title=\"George H. W. Bush\">George H. W. Bush</a>\n",
      "<a href=\"/wiki/Bill_Clinton\" title=\"Bill Clinton\">Bill Clinton</a>\n",
      "<a href=\"/wiki/George_W._Bush\" title=\"George W. Bush\">George W. Bush</a>\n",
      "<a href=\"/wiki/Barack_Obama\" title=\"Barack Obama\">Barack Obama</a>\n",
      "<a href=\"/wiki/Donald_Trump\" title=\"Donald Trump\">Donald Trump</a>\n"
     ]
    }
   ],
   "source": [
    "for link in tb.find_all('b'):\n",
    "    name = link.find('a')\n",
    "    print(name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eleemnt title can be extracted from all a tags using the method get_text()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Washington\n",
      "John Adams\n",
      "Thomas Jefferson\n",
      "James Madison\n",
      "James Monroe\n",
      "John Quincy Adams\n",
      "Andrew Jackson\n",
      "Martin Van Buren\n",
      "William Henry Harrison\n",
      "John Tyler\n",
      "James K. Polk\n",
      "Zachary Taylor\n",
      "Millard Fillmore\n",
      "Franklin Pierce\n",
      "James Buchanan\n",
      "Abraham Lincoln\n",
      "Andrew Johnson\n",
      "Ulysses S. Grant\n",
      "Rutherford B. Hayes\n",
      "James A. Garfield\n",
      "Chester A. Arthur\n",
      "Grover Cleveland\n",
      "Benjamin Harrison\n",
      "Grover Cleveland\n",
      "William McKinley\n",
      "Theodore Roosevelt\n",
      "William H. Taft\n",
      "Woodrow Wilson\n",
      "Warren Harding\n",
      "Calvin Coolidge\n",
      "Herbert Hoover\n",
      "Franklin Delano Roosevelt\n",
      "Harry S. Truman\n",
      "Dwight D. Eisenhower\n",
      "John F. Kennedy\n",
      "Lyndon B. Johnson\n",
      "Richard Nixon\n",
      "Gerald Ford\n",
      "Jimmy Carter\n",
      "Ronald Reagan\n",
      "George H. W. Bush\n",
      "Bill Clinton\n",
      "George W. Bush\n",
      "Barack Obama\n",
      "Donald Trump\n"
     ]
    }
   ],
   "source": [
    "for link in tb.find_all('b'):\n",
    "    name = link.find('a') #  find() returns the first item that matches the tag\n",
    "    print(name.get_text('title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Wikipedia Tables using BeautifulSoup and Python\n",
    "https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting web page data  \n",
    "Use the link from wikipedia: https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area  \n",
    "     # The goal is to scrap Wikipedia to find out all the countries in Asia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import requests library.  \n",
    "Requests allows you to send organic, grass-fed HTTP/1.1 requests, without the need for manual labor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a link to variable named website_url\n",
    "# requests.get(url).text will ping a website and return you HTML of the website.\n",
    "website_url = requests.get('https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By using .text() method from Requests, the website_url will be returned as a string object\n",
    "type(website_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>List of Asian countries by area - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgP\n"
     ]
    }
   ],
   "source": [
    "print(website_url[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by reading the source code for a given web page and creating a BeautifulSoup (soup)object with the BeautifulSoup function.   \n",
    "Beautiful Soup is a Python package for parsing HTML and XML documents.   \n",
    "It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.   \n",
    "Prettify() function in BeautifulSoup will enable us to view how the tags are nested in the document.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Different of Parsers\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   List of Asian countries by area - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   document.documentElement.className=\"client-js\";RLCONF={\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"List_of_Asian_countries_by_area\",\"wgTitle\":\"List of Asian countries by a\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(website_url,'lxml')\n",
    "print(soup.prettify()[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you carefully inspect the HTML script all the table contents i.e. names of the countries which we intend to extract is under class Wikitable Sortable.  \n",
    "So our first task is to find class ‘wikitable sortable’ in the HTML script.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_table = soup.find('table',{'class':'wikitable sortable'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under table class ‘wikitable sortable’ we have links with country name as title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to extract all the links within < a >, we will use find_all()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/wiki/Russia\" title=\"Russia\">Russia</a>,\n",
       " <a href=\"#cite_note-russiaTotalAreaByCIA-1\">[1]</a>,\n",
       " <a href=\"/wiki/China\" title=\"China\">China</a>,\n",
       " <a href=\"/wiki/Hong_Kong\" title=\"Hong Kong\">Hong Kong</a>,\n",
       " <a href=\"/wiki/Macau\" title=\"Macau\">Macau</a>,\n",
       " <a href=\"/wiki/India\" title=\"India\">India</a>,\n",
       " <a href=\"#cite_note-2\">[2]</a>,\n",
       " <a href=\"/wiki/Kazakhstan\" title=\"Kazakhstan\">Kazakhstan</a>,\n",
       " <a href=\"/wiki/Saudi_Arabia\" title=\"Saudi Arabia\">Saudi Arabia</a>,\n",
       " <a href=\"/wiki/Iran\" title=\"Iran\">Iran</a>,\n",
       " <a href=\"/wiki/Mongolia\" title=\"Mongolia\">Mongolia</a>,\n",
       " <a href=\"/wiki/Indonesia\" title=\"Indonesia\">Indonesia</a>,\n",
       " <a href=\"/wiki/Pakistan\" title=\"Pakistan\">Pakistan</a>,\n",
       " <a href=\"/wiki/Turkey\" title=\"Turkey\">Turkey</a>,\n",
       " <a href=\"/wiki/Myanmar\" title=\"Myanmar\">Myanmar</a>,\n",
       " <a href=\"/wiki/Afghanistan\" title=\"Afghanistan\">Afghanistan</a>,\n",
       " <a href=\"/wiki/Yemen\" title=\"Yemen\">Yemen</a>,\n",
       " <a href=\"/wiki/Thailand\" title=\"Thailand\">Thailand</a>,\n",
       " <a href=\"/wiki/Turkmenistan\" title=\"Turkmenistan\">Turkmenistan</a>,\n",
       " <a href=\"/wiki/Uzbekistan\" title=\"Uzbekistan\">Uzbekistan</a>,\n",
       " <a href=\"/wiki/Iraq\" title=\"Iraq\">Iraq</a>,\n",
       " <a href=\"/wiki/Japan\" title=\"Japan\">Japan</a>,\n",
       " <a href=\"/wiki/Vietnam\" title=\"Vietnam\">Vietnam</a>,\n",
       " <a href=\"/wiki/Malaysia\" title=\"Malaysia\">Malaysia</a>,\n",
       " <a href=\"/wiki/Oman\" title=\"Oman\">Oman</a>,\n",
       " <a href=\"/wiki/Philippines\" title=\"Philippines\">Philippines</a>,\n",
       " <a href=\"/wiki/Laos\" title=\"Laos\">Laos</a>,\n",
       " <a href=\"/wiki/Kyrgyzstan\" title=\"Kyrgyzstan\">Kyrgyzstan</a>,\n",
       " <a href=\"/wiki/Syria\" title=\"Syria\">Syria</a>,\n",
       " <a href=\"/wiki/Golan_Heights\" title=\"Golan Heights\">Golan Heights</a>,\n",
       " <a href=\"/wiki/Cambodia\" title=\"Cambodia\">Cambodia</a>,\n",
       " <a href=\"/wiki/Bangladesh\" title=\"Bangladesh\">Bangladesh</a>,\n",
       " <a href=\"/wiki/Nepal\" title=\"Nepal\">Nepal</a>,\n",
       " <a href=\"/wiki/Tajikistan\" title=\"Tajikistan\">Tajikistan</a>,\n",
       " <a href=\"/wiki/North_Korea\" title=\"North Korea\">North Korea</a>,\n",
       " <a href=\"/wiki/South_Korea\" title=\"South Korea\">South Korea</a>,\n",
       " <a href=\"/wiki/Jordan\" title=\"Jordan\">Jordan</a>,\n",
       " <a href=\"/wiki/Azerbaijan\" title=\"Azerbaijan\">Azerbaijan</a>,\n",
       " <a href=\"/wiki/United_Arab_Emirates\" title=\"United Arab Emirates\">United Arab Emirates</a>,\n",
       " <a href=\"/wiki/Georgia_(country)\" title=\"Georgia (country)\">Georgia</a>,\n",
       " <a href=\"/wiki/Sri_Lanka\" title=\"Sri Lanka\">Sri Lanka</a>,\n",
       " <a href=\"/wiki/Egypt\" title=\"Egypt\">Egypt</a>,\n",
       " <a href=\"/wiki/Bhutan\" title=\"Bhutan\">Bhutan</a>,\n",
       " <a href=\"/wiki/Taiwan\" title=\"Taiwan\">Taiwan</a>,\n",
       " <a href=\"/wiki/Armenia\" title=\"Armenia\">Armenia</a>,\n",
       " <a href=\"/wiki/Kuwait\" title=\"Kuwait\">Kuwait</a>,\n",
       " <a href=\"/wiki/East_Timor\" title=\"East Timor\">Timor-Leste</a>,\n",
       " <a href=\"/wiki/Qatar\" title=\"Qatar\">Qatar</a>,\n",
       " <a href=\"/wiki/Lebanon\" title=\"Lebanon\">Lebanon</a>,\n",
       " <a href=\"/wiki/Israel\" title=\"Israel\">Israel</a>,\n",
       " <a href=\"/wiki/State_of_Palestine\" title=\"State of Palestine\">Palestine</a>,\n",
       " <a href=\"/wiki/Brunei\" title=\"Brunei\">Brunei</a>,\n",
       " <a href=\"/wiki/Singapore\" title=\"Singapore\">Singapore</a>,\n",
       " <a href=\"/wiki/Bahrain\" title=\"Bahrain\">Bahrain</a>,\n",
       " <a href=\"/wiki/Maldives\" title=\"Maldives\">Maldives</a>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links=My_table.find_all('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of notes with < a > tag were store in the links variable.  \n",
    "Use operator, in, to filter the < a > tag having 'wiki' as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russia',\n",
       " 'China',\n",
       " 'Hong Kong',\n",
       " 'Macau',\n",
       " 'India',\n",
       " 'Kazakhstan',\n",
       " 'Saudi Arabia',\n",
       " 'Iran',\n",
       " 'Mongolia',\n",
       " 'Indonesia',\n",
       " 'Pakistan',\n",
       " 'Turkey',\n",
       " 'Myanmar',\n",
       " 'Afghanistan',\n",
       " 'Yemen',\n",
       " 'Thailand',\n",
       " 'Turkmenistan',\n",
       " 'Uzbekistan',\n",
       " 'Iraq',\n",
       " 'Japan',\n",
       " 'Vietnam',\n",
       " 'Malaysia',\n",
       " 'Oman',\n",
       " 'Philippines',\n",
       " 'Laos',\n",
       " 'Kyrgyzstan',\n",
       " 'Syria',\n",
       " 'Golan Heights',\n",
       " 'Cambodia',\n",
       " 'Bangladesh',\n",
       " 'Nepal',\n",
       " 'Tajikistan',\n",
       " 'North Korea',\n",
       " 'South Korea',\n",
       " 'Jordan',\n",
       " 'Azerbaijan',\n",
       " 'United Arab Emirates',\n",
       " 'Georgia (country)',\n",
       " 'Sri Lanka',\n",
       " 'Egypt',\n",
       " 'Bhutan',\n",
       " 'Taiwan',\n",
       " 'Armenia',\n",
       " 'Kuwait',\n",
       " 'East Timor',\n",
       " 'Qatar',\n",
       " 'Lebanon',\n",
       " 'Israel',\n",
       " 'State of Palestine',\n",
       " 'Brunei',\n",
       " 'Singapore',\n",
       " 'Bahrain',\n",
       " 'Maldives']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries=[]                     # create an empty list to append the country name in for loop\n",
    "for link in links:\n",
    "    if 'wiki' in str(link):      # find the tage with strings having 'wiki'\n",
    "        countries.append(link.get('title'))  # alternative: use get_text() method\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the list countries into Pandas DataFrame to work in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hong Kong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Macau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country\n",
       "0     Russia\n",
       "1      China\n",
       "2  Hong Kong\n",
       "3      Macau\n",
       "4      India"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "df['Country']=countries\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping HTML Tables with Python\n",
    "https://towardsdatascience.com/web-scraping-html-tables-with-python-c9baba21059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting web page data  \n",
    "    # The goal is to try scraping the online Pokemon Database \n",
    "(http://pokemondb.net/pokedex/all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect HTML\n",
    "Before moving forward, we need to understand the structure of the website we wish to scrape.  \n",
    "This can be done by clicking right-clicking the element we wish to scrape and then hitting “Inspect”. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Libraries\n",
    "We will need requests for getting the HTML contents of the website and lxml.html for parsing the relevant fields. Finally, we will store the data on a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests          # to get a response from a url\n",
    "import lxml.html as lh   # lxml parser\n",
    "import pandas as pd      # pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape Table Cells\n",
    "The code below allows us to get the Pokemon stats data of the HTML table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the url to an url variable\n",
    "url='https://pokemondb.net/pokedex/all'\n",
    "\n",
    "#Create a handle, page, to handle the contents of the website\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# It is always good to check the http response status code from Requests \n",
    "print(page.status_code)   # This should print 200\n",
    "# print(page[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n<meta charset=\"utf-8\">\\n<title>Pok\\xc3\\xa9mon Pok\\xc3\\xa9dex: list of Pok\\xc3\\xa9mon with stats | Pok\\xc3\\xa9mon Database</title>\\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\">\\n<link rel=\"preconnect\" href=\"https://img.pokemondb.net\">\\n<link rel=\"stylesheet\" href=\"/static/css/pokemondb-e614e67e0f.css\">\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n<meta property=\"og:description\" name=\"description\" content=\"The Pok\\xc3\\xa9dex contains detailed stats for eve'\n"
     ]
    }
   ],
   "source": [
    "print(page.content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using beautifulsoup, use fromstring() method from lxml.html module to create a html structure object\n",
    "#Store the contents of the website under doc\n",
    "doc = lh.fromstring(page.content)  # <lxml.html.HtmlElement> / <Element html at 0x121281688>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lxml.html.HtmlElement"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  < table > tag\n",
    "The < table > tag defines an HTML table.  \n",
    "An HTML table consists of the < table > element and one or more < tr >, < th >, and < td > elements.  \n",
    "The < tr > element defines a table row,  \n",
    "The < th > element defines a table header,  \n",
    "and the < td > element defines a table cell.  \n",
    "\n",
    "A more complex HTML table may also include < caption >, < col >, < colgroup >, < thead >, < tfoot >, and < tbody > elements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:   \n",
    "< table>  \n",
    "....< tr>  \n",
    "........< th>Month< /th>  \n",
    "........< th>Savings< /th>  \n",
    "....< / tr>  \n",
    "....< tr>  \n",
    "........< td> January< / td>  \n",
    "........< td> 100< / td>  \n",
    "....< / tr>  \n",
    "....< tr>  \n",
    "........< td>February< /td>  \n",
    "........< td> 80< /td>  \n",
    "....< /tr>  \n",
    "< /table>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse data that are stored between <tr>..</tr> of HTML by using xpath() from lxml module\n",
    "# the xpath() method will return a list\n",
    "tr_elements = doc.xpath('//tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The new variable is a list of html structure objects\n",
    "type(tr_elements)  # <list>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sanity check, ensure that all the rows have the same width.   \n",
    "If not, we probably got something more than just the table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the length of the first 12 rows\n",
    "[len(T) for T in tr_elements[:12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all our rows have exactly 10 columns. This means all the data collected on tr_elements are from the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parse Table Header\n",
    "Next, let’s parse the first row as our header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tr_elements' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b375793d8f15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#For each row, store each first element (header) and an empty list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_elements\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:2d} : {:s}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tr_elements' is not defined"
     ]
    }
   ],
   "source": [
    "#Create empty list\n",
    "col=[]\n",
    "#For each row, store each first element (header) and an empty list\n",
    "for i,t in enumerate(tr_elements[0]):\n",
    "    name = t.text_content()\n",
    "    print('{:2d} : {:s}'.format(i+1,name))\n",
    "    col.append((name,[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', []),\n",
       " ('Name', []),\n",
       " ('Type', []),\n",
       " ('Total', []),\n",
       " ('HP', []),\n",
       " ('Attack', []),\n",
       " ('Defense', []),\n",
       " ('Sp. Atk', []),\n",
       " ('Sp. Def', []),\n",
       " ('Speed', [])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a list of tuples that contains column header and empty lists.  \n",
    "We are going to iterate each row to fill the cells from the table to the empty lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Pandas DataFrame  \n",
    "Each header is appended to a tuple along with an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since our first row is the header, data is stored on the second row onwards\n",
    "for j in range(1,len(tr_elements)):\n",
    "    # T is our j'th row\n",
    "    T=tr_elements[j]\n",
    "    \n",
    "    # If row is not of size 10, the //tr data is not from our table \n",
    "    if len(T)!=10:\n",
    "        break\n",
    "    \n",
    "    #i is the index counter of our column\n",
    "    i=0\n",
    "    \n",
    "    #Iterate through each element of the row\n",
    "    for t in T.iterchildren():\n",
    "        data=t.text_content() \n",
    "        #Check if row is empty\n",
    "        if i>0:\n",
    "        #Convert any numerical value to integers\n",
    "        # The table on the web has the numeric value in integer, instead of float\n",
    "        # Try to convert the data into numeric\n",
    "        # If it cannot be converted, use the except and pass keep the data as a string\n",
    "            try:\n",
    "                data=int(data)\n",
    "            except:\n",
    "                pass\n",
    "        #Append the data to the empty list of the i'th column\n",
    "        col[i][1].append(data) \n",
    "        #Increment i for the next column\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[926, 926, 926, 926, 926, 926, 926, 926, 926, 926]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(cell) for (colname,cell) in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! This shows that each of our 10 columns has exactly 926 values.  \n",
    "Now we are ready to create the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict={colname : column for (colname , column) in col}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Total</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass Poison</td>\n",
       "      <td>318</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass Poison</td>\n",
       "      <td>405</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass Poison</td>\n",
       "      <td>525</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003</td>\n",
       "      <td>Venusaur Mega Venusaur</td>\n",
       "      <td>Grass Poison</td>\n",
       "      <td>625</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>309</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     #                    Name          Type  Total  HP  Attack  Defense  \\\n",
       "0  001               Bulbasaur  Grass Poison    318  45      49       49   \n",
       "1  002                 Ivysaur  Grass Poison    405  60      62       63   \n",
       "2  003                Venusaur  Grass Poison    525  80      82       83   \n",
       "3  003  Venusaur Mega Venusaur  Grass Poison    625  80     100      123   \n",
       "4  004              Charmander         Fire     309  39      52       43   \n",
       "\n",
       "   Sp. Atk  Sp. Def  Speed  \n",
       "0       65       65     45  \n",
       "1       80       80     60  \n",
       "2      100      100     80  \n",
       "3      122      120     80  \n",
       "4       60       50     65  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df=pd.DataFrame(Dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wola! we have scraped the pokedex table from the web :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
